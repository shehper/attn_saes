{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "lr_decay_steps: 5000\n",
      "l1_warmup_steps: 1250\n",
      "Run name: 16384-L1-2-LR-5e-05-Tokens-1.024e+08\n",
      "n_tokens_per_buffer (millions): 1.048576\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.001024\n",
      "Total training steps: 25000\n",
      "Total wandb updates: 500\n",
      "n_tokens_per_feature_sampling_window (millions): 4194.304\n",
      "n_tokens_per_dead_feature_window (millions): 4194.304\n",
      "We will reset the sparsity calculation 25 times.\n",
      "Number tokens in sparsity calculation window: 4.10e+06\n",
      "Total Training Tokens: 102400000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from sae_lens import LanguageModelSAERunnerConfig, SAETrainingRunner\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# default config values\n",
    "model_name = \"gelu-2l\" \n",
    "dataset_path = \"NeelNanda/c4-code-tokenized-2b\"\n",
    "\n",
    "total_training_steps = 25_000 \n",
    "batch_size = 4096 \n",
    "new_cached_activations_path = (\n",
    "    f\"./cached_activations/{model_name}/{dataset_path}/{total_training_steps}\"\n",
    ")\n",
    "\n",
    "hook_layer=1\n",
    "hook_name=f\"blocks.{hook_layer}.attn.hook_z\"\n",
    "hook_head_index = None\n",
    "l1_coefficients = [2, 5, 10]\n",
    "d_in= 512\n",
    "expansion_factor = 32\n",
    "\n",
    "total_training_tokens = total_training_steps * batch_size\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "lr_warm_up_steps = 0\n",
    "lr_decay_steps = total_training_steps // 5  # 20% of training steps.\n",
    "print(f\"lr_decay_steps: {lr_decay_steps}\")\n",
    "l1_warmup_steps = total_training_steps // 20  # 5% of training steps.\n",
    "print(f\"l1_warmup_steps: {l1_warmup_steps}\")\n",
    "log_to_wandb = True\n",
    "\n",
    "def save_init_weights(runner):\n",
    "    init_weights = {}\n",
    "    for n, p in runner.sae.named_parameters():\n",
    "        init_weights[n] = p.data.cpu().clone()\n",
    "    return init_weights\n",
    "\n",
    "for l1_coefficient in l1_coefficients:\n",
    "    cfg = LanguageModelSAERunnerConfig(\n",
    "        # Pick a tiny model to make this easier.\n",
    "        model_name=model_name, \n",
    "        ## MLP Layer 0 ##\n",
    "        hook_name=hook_name, \n",
    "        hook_layer=hook_layer, \n",
    "        hook_head_index=hook_head_index,\n",
    "        d_in=d_in,\n",
    "        dataset_path=dataset_path,\n",
    "        streaming=True, # pre-download the token dataset\n",
    "        context_size=1024,\n",
    "        is_dataset_tokenized=True,\n",
    "        prepend_bos=True,\n",
    "\n",
    "        n_heads=8, # TODO: replace the hack\n",
    "\n",
    "        # How big do we want our SAE to be?\n",
    "        expansion_factor=expansion_factor,\n",
    "        # Dataset / Activation Store\n",
    "        # When we do a proper test\n",
    "        # training_tokens= 820_000_000, # 200k steps * 4096 batch size ~ 820M tokens (doable overnight on an A100)\n",
    "        # For now.\n",
    "        use_cached_activations=False,\n",
    "        #cached_activations_path=\"./gelu-2l\",\n",
    "        training_tokens=total_training_tokens,  # For initial testing I think this is a good number.\n",
    "        train_batch_size_tokens=batch_size,\n",
    "        # Loss Function\n",
    "        ## Reconstruction Coefficient.\n",
    "        mse_loss_normalization=None,  # MSE Loss Normalization is not mentioned (so we use stanrd MSE Loss). But not we take an average over the batch.\n",
    "        ## Anthropic does not mention using an Lp norm other than L1.\n",
    "        l1_coefficient=l1_coefficient,\n",
    "        lp_norm=1.0,\n",
    "        # Instead, they multiply the L1 loss contribution\n",
    "        # from each feature of the activations by the decoder norm of the corresponding feature.\n",
    "        scale_sparsity_penalty_by_decoder_norm=True,\n",
    "        # Learning Rate\n",
    "        lr_scheduler_name=\"constant\",  # we set this independently of warmup and decay steps. # TODO: understand why it's constant\n",
    "        l1_warm_up_steps=l1_warmup_steps,\n",
    "        lr_warm_up_steps=lr_warm_up_steps,\n",
    "        lr_decay_steps=lr_warm_up_steps,\n",
    "        ## No ghost grad term.\n",
    "        use_ghost_grads=False,\n",
    "        # Initialization / Architecture\n",
    "        architecture=\"block_diag\",\n",
    "        apply_b_dec_to_input=False,\n",
    "        # encoder bias zero's. (I'm not sure what it is by default now)\n",
    "        # decoder bias zero's.\n",
    "        b_dec_init_method=\"zeros\",\n",
    "        normalize_sae_decoder=False,\n",
    "        decoder_heuristic_init=True,\n",
    "        init_encoder_as_decoder_transpose=True,\n",
    "        # Optimizer\n",
    "        lr=5e-5,\n",
    "        ## adam optimizer has no weight decay by default so worry about this.\n",
    "        adam_beta1=0.9,\n",
    "        adam_beta2=0.999,\n",
    "        # Buffer details won't matter in we cache / shuffle our activations ahead of time.\n",
    "        n_batches_in_buffer=64,\n",
    "        store_batch_size_prompts=16,\n",
    "        normalize_activations=\"expected_average_only_in\", # TODO: not sure what's the best choice # Activation Normalization Strategy. Either none, expected_average_only_in, or constant_norm_rescale.\n",
    "        # Feature Store\n",
    "        feature_sampling_window=1000,\n",
    "        dead_feature_window=1000,\n",
    "        dead_feature_threshold=1e-4,\n",
    "        # WANDB\n",
    "        log_to_wandb=log_to_wandb,  # always use wandb unless you are just testing code.\n",
    "        wandb_project=f\"{model_name}-attn-{hook_layer}-sae\",\n",
    "        wandb_log_frequency=50,\n",
    "        eval_every_n_wandb_logs=10,\n",
    "        # Misc\n",
    "        device=device,\n",
    "        seed=42,\n",
    "        n_checkpoints=2,\n",
    "        checkpoint_path=\"checkpoints\",\n",
    "        dtype=\"float32\",\n",
    "    )\n",
    "\n",
    "    print(f\"Total Training Tokens: {total_training_tokens}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gelu-2l into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51f58eaa7f6e4dc9aa56be9fd7d6fe16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshehper\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/storage/attn_saes/wandb/run-20240706_201616-0dxz3yxo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shehper/gelu-2l-attn-1-sae/runs/0dxz3yxo' target=\"_blank\">16384-L1-2-LR-5e-05-Tokens-1.024e+08</a></strong> to <a href='https://wandb.ai/shehper/gelu-2l-attn-1-sae' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shehper/gelu-2l-attn-1-sae' target=\"_blank\">https://wandb.ai/shehper/gelu-2l-attn-1-sae</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shehper/gelu-2l-attn-1-sae/runs/0dxz3yxo' target=\"_blank\">https://wandb.ai/shehper/gelu-2l-attn-1-sae/runs/0dxz3yxo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Estimating norm scaling factor: 100%|██████████| 1000/1000 [00:36<00:00, 27.77it/s]\n",
      "1000| MSE Loss 105.488 | L1 159.019:   4%|▍         | 4096000/102400000 [01:46<31:30, 52008.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interrupted, saving progress\n",
      "done saving\n"
     ]
    },
    {
     "ename": "InterruptedException",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInterruptedException\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23287/943607361.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrunner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSAETrainingRunner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minit_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave_init_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/storage/SAELens/sae_lens/sae_training_runner.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compile_if_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0msae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_trainer_with_interruption_handling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_to_wandb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/storage/SAELens/sae_lens/sae_training_runner.py\u001b[0m in \u001b[0;36mrun_trainer_with_interruption_handling\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;31m# train SAE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0msae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInterruptedException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/storage/SAELens/sae_lens/training/sae_trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_training_tokens\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch_size_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             \u001b[0mstep_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msae\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msae_in\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_acts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_to_wandb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/storage/SAELens/sae_lens/training/sae_trainer.py\u001b[0m in \u001b[0;36m_train_step\u001b[0;34m(self, sae, sae_in)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munscale_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# needed to clip correctly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;31m# TODO: Work out if grad norm clipping should be in config / how to test it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# just ctx.optimizer.step() if not autocasting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrouped_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mforeach\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mforeach\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_has_foreach_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_foreach_mul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_coef_clamped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-overload]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mforeach\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'foreach=True was passed, but can\\'t use the foreach API on {device.type} tensors'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/storage/SAELens/sae_lens/sae_training_runner.py\u001b[0m in \u001b[0;36minterrupt_callback\u001b[0;34m(sig_num, stack_frame)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minterrupt_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig_num\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_frame\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mInterruptedException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInterruptedException\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000| MSE Loss 105.488 | L1 159.019:   4%|▍         | 4096000/102400000 [01:57<31:30, 52008.24it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "runner = SAETrainingRunner(cfg)\n",
    "init_weights = save_init_weights(runner=runner)\n",
    "runner.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_weights = {}\n",
    "for n, p in runner.sae.named_parameters():\n",
    "    init_weights[n] = p.data.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0956,  0.1038, -0.0293,  ..., -0.0853,  0.0385, -0.0430],\n",
       "        [ 0.0383, -0.0260,  0.1037,  ..., -0.0729, -0.0428, -0.0987],\n",
       "        [ 0.1048, -0.0248,  0.1075,  ...,  0.0134, -0.0221, -0.0373],\n",
       "        ...,\n",
       "        [ 0.0353,  0.0179, -0.0835,  ...,  0.1018,  0.0542, -0.0653],\n",
       "        [ 0.0442, -0.0610, -0.0666,  ..., -0.0071,  0.1038,  0.0466],\n",
       "        [-0.0996, -0.0305, -0.0282,  ..., -0.0201, -0.0599,  0.0170]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_weights['enc_blocks.0.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0956,  0.0383,  0.1048,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.1038, -0.0260, -0.0248,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.0293,  0.1037,  0.1075,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0785,  0.0362, -0.0852],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ..., -0.1197, -0.0665,  0.0464],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ..., -0.0890,  0.0489, -0.0697]],\n",
       "       device='cuda:0', grad_fn=<CloneBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_weights[\"W_enc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0211,  0.0075, -0.0008,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.0076, -0.0160, -0.0113,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0147, -0.0185,  0.0202,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ..., -0.0030,  0.0142, -0.0211],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0172, -0.0179,  0.0185],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ..., -0.0214,  0.0196,  0.0009]],\n",
       "       device='cuda:0', grad_fn=<CloneBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_weights[\"W_dec\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0597,  0.0812, -0.0701,  ...,  0.0253, -0.1019, -0.1229],\n",
       "       device='cuda:0', grad_fn=<CloneBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_weights[\"b_enc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.3798e-03, -1.4406e-02,  6.6600e-03,  1.7949e-02, -2.0928e-02,\n",
       "        -1.7106e-02, -6.7341e-03, -1.4137e-02,  1.8258e-02,  7.8433e-03,\n",
       "        -1.6477e-02, -6.5140e-03,  6.9009e-03, -6.8866e-04,  2.0495e-02,\n",
       "        -7.3730e-03,  1.2815e-03, -4.3822e-03, -4.4309e-04,  1.5135e-02,\n",
       "        -2.4795e-03, -1.4174e-02,  2.8641e-03, -1.9982e-02,  9.7295e-03,\n",
       "        -8.5921e-03, -1.9043e-02,  1.8825e-02,  2.1587e-02,  9.4444e-03,\n",
       "         1.0079e-02, -9.3242e-03, -1.9717e-02, -1.8062e-02, -8.3509e-04,\n",
       "         1.3807e-02,  3.9873e-03,  1.5570e-02, -6.1561e-03, -3.0117e-03,\n",
       "         1.1046e-03,  1.3812e-02,  1.8806e-03,  2.1446e-02, -1.1533e-02,\n",
       "         8.5836e-04,  1.5602e-02,  1.7486e-02, -4.0159e-03,  8.7399e-04,\n",
       "        -5.6304e-03,  6.2776e-03,  1.8152e-02,  1.1937e-02, -1.7574e-02,\n",
       "        -1.2467e-02, -8.6007e-03, -1.5669e-03,  1.9530e-03, -1.4182e-02,\n",
       "         1.6515e-02, -8.2939e-03,  4.3075e-03, -7.4483e-03, -9.9832e-03,\n",
       "         1.5400e-02, -7.8276e-03,  1.8333e-02,  1.6065e-02,  1.5794e-02,\n",
       "        -1.0174e-02,  9.8620e-03,  9.9325e-03,  1.0042e-02,  4.4841e-03,\n",
       "        -5.2967e-03,  1.5967e-02,  6.3166e-03, -1.6304e-02,  1.8903e-03,\n",
       "         1.2348e-02,  1.5531e-02,  8.5566e-03, -1.0786e-02,  1.8300e-03,\n",
       "        -1.5902e-02,  2.5025e-03, -6.1975e-03, -1.7365e-02, -2.8119e-03,\n",
       "         2.1122e-03,  9.5513e-03,  1.0145e-02, -8.0543e-03, -1.9302e-02,\n",
       "        -1.1572e-02,  2.3927e-03,  4.7062e-03,  9.8842e-03,  1.4323e-02,\n",
       "         3.0709e-04,  9.3984e-03, -6.2569e-03, -2.0111e-02, -6.1484e-03,\n",
       "        -5.3214e-03, -1.6251e-02, -1.4313e-02,  1.9002e-02, -1.8564e-02,\n",
       "         2.1892e-02,  4.6396e-03,  1.5375e-02,  1.9315e-03, -2.2052e-02,\n",
       "        -9.1604e-03,  1.2456e-02,  2.1052e-02,  1.1110e-02, -1.3480e-02,\n",
       "        -8.4249e-03, -5.8377e-03, -2.0687e-02, -1.0632e-02,  4.2289e-03,\n",
       "        -4.0224e-03,  9.8036e-03, -5.2902e-03,  3.2402e-03,  1.6000e-02,\n",
       "         1.3813e-02, -2.2331e-03, -9.8773e-03,  2.0342e-02,  5.2654e-03,\n",
       "         7.7689e-03,  2.0118e-02, -1.9179e-02,  1.4944e-02, -1.6937e-02,\n",
       "        -3.8523e-03,  1.6567e-02,  9.0003e-03,  2.1995e-02, -3.7272e-03,\n",
       "         9.5056e-03, -9.5755e-03, -2.6574e-03, -6.5964e-03, -2.0274e-02,\n",
       "        -1.9307e-02, -1.6825e-02,  6.8318e-03,  2.1220e-02, -1.0944e-02,\n",
       "        -1.6717e-02, -1.0474e-02,  1.6347e-03,  1.5871e-02,  8.3941e-03,\n",
       "        -9.5250e-03,  1.0702e-02,  3.0422e-03,  4.0025e-03, -2.7169e-03,\n",
       "         2.3218e-05, -1.9618e-02,  8.2327e-03,  1.1971e-02, -7.8233e-03,\n",
       "         1.2879e-02,  1.9737e-02, -4.4564e-03, -9.4954e-03, -8.5579e-03,\n",
       "        -1.6524e-02,  4.7672e-03, -1.7111e-02,  4.7194e-03,  2.0389e-02,\n",
       "         3.1561e-03,  1.4239e-02, -8.4904e-03,  1.8696e-02,  9.1066e-03,\n",
       "         1.7961e-02,  1.3336e-02,  2.1227e-02, -6.8242e-03,  1.9864e-02,\n",
       "        -1.2982e-02,  6.0987e-03, -1.7159e-04,  1.0737e-02,  1.9988e-02,\n",
       "        -1.9674e-02, -1.4067e-02,  2.1439e-02, -1.5247e-02, -1.6049e-02,\n",
       "        -2.0818e-02,  1.9314e-02, -1.8986e-02, -1.0852e-03, -9.6789e-03,\n",
       "         9.9067e-03, -1.7160e-02,  2.0734e-02,  1.4270e-02, -5.9109e-03,\n",
       "        -1.7561e-03, -1.4378e-02,  5.9075e-03,  7.9174e-03, -1.2532e-03,\n",
       "         3.0260e-03,  1.8184e-02,  2.0534e-02,  1.6875e-02,  1.9888e-02,\n",
       "         2.0053e-03, -9.6780e-03, -2.1501e-02,  1.5586e-03, -1.3306e-02,\n",
       "         1.3959e-02,  1.9013e-02, -1.9707e-03, -1.9667e-02,  1.5015e-02,\n",
       "        -1.6600e-03, -4.4312e-03, -2.0383e-02, -1.5306e-02, -1.4258e-02,\n",
       "         1.5534e-02,  1.2864e-02,  1.9599e-03, -1.6064e-02,  1.6158e-02,\n",
       "        -5.5141e-03,  1.9992e-02,  7.2639e-03, -1.2270e-02, -8.0061e-03,\n",
       "        -9.2400e-03,  1.0232e-03, -5.5459e-03,  1.5202e-02,  1.7318e-02,\n",
       "         4.3657e-03,  3.3346e-03, -5.1637e-03, -1.9638e-03, -2.0044e-02,\n",
       "        -2.4226e-03, -7.2147e-03,  1.6145e-02, -2.1189e-02, -1.5856e-02,\n",
       "         9.3124e-03,  5.8605e-03, -5.6209e-03,  1.9574e-02,  9.9533e-03,\n",
       "        -1.9006e-02, -1.8161e-02,  1.5324e-02, -1.0391e-03, -2.2002e-02,\n",
       "        -1.6478e-02,  5.9831e-03,  1.2950e-02,  9.7132e-03,  9.5872e-03,\n",
       "        -6.1155e-03, -7.2511e-03, -1.7441e-02, -1.8641e-03, -3.3536e-03,\n",
       "         4.2144e-03, -1.8365e-02, -2.1714e-02, -5.5033e-03, -1.8914e-02,\n",
       "        -5.3334e-03, -2.0262e-02, -1.2233e-02, -5.8446e-03,  1.2813e-02,\n",
       "        -1.2751e-02,  1.7549e-02, -5.6615e-03,  1.7716e-02,  3.7918e-03,\n",
       "         1.8072e-02,  7.1753e-03,  1.3258e-02,  9.5446e-03, -2.2465e-03,\n",
       "         5.4168e-03, -1.1699e-02,  9.9525e-03, -4.6533e-03, -2.6558e-03,\n",
       "         3.6673e-03, -1.1969e-02, -1.3481e-03,  2.1016e-03, -1.8687e-02,\n",
       "        -8.9294e-03, -1.3973e-02, -8.5371e-03,  1.5835e-02, -1.5910e-02,\n",
       "        -4.5298e-03, -1.5502e-02, -1.0866e-03,  7.7001e-03,  2.1108e-02,\n",
       "         1.9575e-02, -2.1446e-02, -4.0207e-03,  4.7460e-03,  1.8204e-02,\n",
       "        -1.6487e-02,  1.0613e-03, -7.6683e-03,  1.6119e-03, -9.2235e-03,\n",
       "        -1.4506e-02, -9.4459e-03,  1.2910e-02, -2.1640e-03,  2.8931e-04,\n",
       "         1.3879e-02, -1.8240e-04, -9.7415e-03, -3.5712e-03,  2.6609e-03,\n",
       "         1.1811e-02, -1.3570e-02,  4.1479e-03,  9.5230e-03,  1.9572e-02,\n",
       "        -1.2814e-02,  3.8382e-03, -2.0996e-02,  9.7720e-03, -4.7013e-03,\n",
       "         2.0195e-02, -4.9933e-03,  4.5380e-04, -5.6294e-03, -1.0777e-02,\n",
       "        -1.9261e-02,  4.5820e-04,  1.2266e-02,  2.9061e-03, -4.5473e-03,\n",
       "         1.6227e-02, -1.2922e-02,  1.7519e-02,  9.1659e-03, -1.3584e-02,\n",
       "        -2.0271e-02,  8.3985e-03, -1.4149e-03,  9.7899e-03,  3.2304e-03,\n",
       "         2.1818e-02, -1.9427e-02, -1.1162e-02,  1.8065e-02, -9.9713e-03,\n",
       "         1.3727e-02, -8.0284e-03,  2.2017e-02, -5.7520e-03, -1.5552e-02,\n",
       "         1.2710e-02, -1.4272e-02, -1.5429e-02,  3.4578e-03, -1.2700e-02,\n",
       "        -1.8065e-02,  4.7884e-05,  1.3811e-02, -1.8473e-02,  1.6903e-03,\n",
       "         1.6596e-02,  8.9170e-03,  2.2040e-03, -2.0003e-02,  4.2393e-03,\n",
       "         2.0441e-02, -1.6321e-02, -1.8887e-02, -1.2713e-02,  8.0847e-03,\n",
       "        -3.9080e-03,  4.0378e-03,  1.5901e-02,  1.5924e-02, -2.6757e-04,\n",
       "        -1.7951e-02,  1.1294e-02, -1.5901e-02,  1.1642e-02, -5.7592e-03,\n",
       "         1.9497e-02, -2.1072e-02,  1.2698e-03, -1.0614e-02,  3.9841e-03,\n",
       "        -2.3999e-03,  1.3248e-02, -1.9742e-02,  1.9520e-02,  1.9328e-02,\n",
       "         1.9681e-02, -2.8547e-03,  3.1495e-03, -9.8730e-03, -1.7304e-02,\n",
       "        -2.0303e-02, -2.0322e-02, -2.3543e-03,  8.6592e-03, -2.7236e-04,\n",
       "        -2.0222e-02,  8.4615e-03,  2.1180e-03,  4.2674e-03,  2.5592e-03,\n",
       "         2.0719e-02, -1.1642e-02,  9.3864e-03,  1.4032e-02,  7.8961e-04,\n",
       "        -1.3747e-02,  1.0109e-02, -1.3050e-02,  1.5522e-03, -1.5257e-02,\n",
       "        -1.5443e-02, -5.5612e-03,  1.7496e-02, -2.0138e-02,  7.2611e-03,\n",
       "         4.2105e-05,  5.7584e-03, -1.9806e-02,  1.2392e-02, -1.1605e-02,\n",
       "         2.2086e-02, -1.3747e-03,  2.0813e-02, -2.1645e-02,  7.5019e-03,\n",
       "        -1.1381e-02,  1.0674e-02,  5.5110e-03, -2.1907e-02, -1.1889e-02,\n",
       "         1.3309e-02, -9.1529e-03,  1.1930e-02, -7.7401e-03, -7.1287e-03,\n",
       "        -5.4505e-03, -1.0425e-02, -1.0578e-02, -1.5516e-03,  1.6415e-02,\n",
       "         4.9655e-03,  1.4018e-02,  1.1418e-02,  1.6611e-02, -1.5351e-02,\n",
       "         2.4973e-03, -1.8569e-02, -1.7925e-02, -1.2065e-02, -5.8810e-03,\n",
       "        -4.1335e-03,  1.3426e-02, -8.2989e-03, -1.6010e-02,  1.8705e-03,\n",
       "         2.1569e-02,  1.4459e-02,  1.1350e-02, -6.5487e-04,  6.4808e-03,\n",
       "        -1.9302e-02, -1.5611e-02,  2.1814e-03,  8.9492e-03,  1.6430e-02,\n",
       "         1.4322e-02,  2.6433e-03,  2.9280e-03, -3.9739e-03,  1.2040e-02,\n",
       "        -6.3524e-03,  7.5688e-03, -1.8060e-02,  1.4731e-02,  2.0151e-02,\n",
       "        -1.6987e-03,  6.2111e-03], device='cuda:0', grad_fn=<CloneBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_weights[\"b_dec\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initiate `runner` by instantiating `SAETrainingRunner(cfg)`. This class is defined in `sae_training_runner.py`.\n",
    "\n",
    "It has an attribute `sae` which is an instance of `TrainingSAE` defined in `sae_lens.training.training_sae`.\n",
    "\n",
    "In the `__init__` method of `TrainingSAE`, I have placed\n",
    "\n",
    "```\n",
    "if self.cfg.architecture == \"block_diag\":\n",
    "    self.encode_with_hidden_pre_fn = self.encode_block_diag\n",
    "    self.decode_fn = self.decode_block_diag\n",
    "```\n",
    "\n",
    "We train the SAE by calling `runner.run()`. This defines `trainer` to be an instance of `SAETrainer`, which is defined in `sae_lens.training.sae_trainer`.\n",
    "\n",
    "Training is done by calling `trainer.fit()`, which calls `self._train_step` method. This method calls `self.sae.training_forward_pass` which is a method of the `TrainingSAE` class. It also calls `self._run_and_log_evals`, which calls `run_evals` from `sae_lens.evals.py`.\n",
    "\n",
    "My job is to make sure that both `TrainingSAE.training_forward_pass` and `run_evals` are consistent with the block diagonal architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Estimating norm scaling factor: 100%|██████████| 1000/1000 [00:34<00:00, 28.85it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TrainingSAE' object has no attribute 'W_dec'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6249/3926302587.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/storage/SAELens/sae_lens/sae_training_runner.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compile_if_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0msae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_trainer_with_interruption_handling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_to_wandb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/storage/SAELens/sae_lens/sae_training_runner.py\u001b[0m in \u001b[0;36mrun_trainer_with_interruption_handling\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;31m# train SAE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0msae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInterruptedException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/storage/SAELens/sae_lens/training/sae_trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_training_tokens\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch_size_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             \u001b[0mstep_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msae\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msae_in\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_acts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_to_wandb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/storage/SAELens/sae_lens/training/sae_trainer.py\u001b[0m in \u001b[0;36m_train_step\u001b[0;34m(self, sae, sae_in)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast_if_enabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             train_step_output = self.sae.training_forward_pass(\n\u001b[0m\u001b[1;32m    217\u001b[0m                 \u001b[0msae_in\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msae_in\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0mdead_neuron_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdead_neurons\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/storage/SAELens/sae_lens/training/training_sae.py\u001b[0m in \u001b[0;36mtraining_forward_pass\u001b[0;34m(self, sae_in, current_l1_coefficient, dead_neuron_mask)\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m             \u001b[0;31m# default SAE sparsity loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m             \u001b[0mweighted_feature_acts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_acts\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_dec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m             sparsity = weighted_feature_acts.norm(\n\u001b[1;32m    369\u001b[0m                 \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlp_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1614\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m   1615\u001b[0m             type(self).__name__, name))\n\u001b[1;32m   1616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TrainingSAE' object has no attribute 'W_dec'"
     ]
    }
   ],
   "source": [
    "runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "type(torch.cat([nn.Parameter(torch.randn(2, )), nn.Parameter(torch.randn(2, ))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_in = 16\n",
    "n_blocks = 8\n",
    "n_latents = 32\n",
    "device = \"cuda\"\n",
    "dec_blocks = nn.ModuleDict({str(i): nn.Linear(n_latents // n_blocks, n_in // n_blocks).to(device=device) for i in range(n_blocks)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1662, -0.2651, -0.4207, -0.2487],\n",
       "        [-0.3440,  0.3033,  0.1173, -0.0140]], device='cuda:0',\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_blocks[\"0\"].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_dec = torch.block_diag(*[dec_blocks[str(i)].weight.t() for i in range(n_blocks)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 16])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_dec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3820, 0.4028, 0.4367, 0.2491, 0.3898, 0.1922, 0.4765, 0.5371, 0.4102,\n",
       "        0.1023, 0.1718, 0.4046, 0.3490, 0.2021, 0.4888, 0.2652, 0.1489, 0.3086,\n",
       "        0.2403, 0.4287, 0.1808, 0.3064, 0.1680, 0.3593, 0.4716, 0.4752, 0.4245,\n",
       "        0.4255, 0.4611, 0.2527, 0.2090, 0.4547], device='cuda:0',\n",
       "       grad_fn=<LinalgVectorNormBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_dec.norm(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3820, 0.4028, 0.4367, 0.2491], device='cuda:0',\n",
       "       grad_fn=<LinalgVectorNormBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_blocks[\"0\"].weight.t().norm(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([nn.Parameter(2, 4), nn.Parameter(2, 4)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
