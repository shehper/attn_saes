{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of this code is borrowed from an [SAELens tutorial](https://github.com/jbloomAus/SAELens/blob/main/tutorials/basic_loading_and_analysing.ipynb), and some of it is borrowed from Connor Kissane's attention-output-saes [repository](https://github.com/ckkissane/attention-output-saes). Thank you to Connor and to the contributors of SAELens for their contributions and for making their code public!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab # type: ignore\n",
    "    from google.colab import output\n",
    "    COLAB = True\n",
    "    %pip install sae-lens transformer-lens\n",
    "except:\n",
    "    COLAB = False\n",
    "    from IPython import get_ipython # type: ignore\n",
    "    ipython = get_ipython(); assert ipython is not None\n",
    "    ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "    ipython.run_line_magic(\"autoreload\", \"2\")\n",
    "\n",
    "# Standard imports\n",
    "import os\n",
    "import torch\n",
    "\n",
    "torch.set_grad_enabled(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# For the most part I'll try to import functions and classes near where they are used\n",
    "# to make it clear where they come from.\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset  \n",
    "from transformer_lens import HookedTransformer\n",
    "from sae_lens import SAE\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\", device = device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and pre-processing the SAE weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the block_diag_sae [branch](https://github.com/shehper/SAELens/tree/block_diag_sae) of SAELens used to train these models, an SAE did not have `W_dec` or `W_enc` as its attributes. Instead it had `dec_blocks` and `enc_blocks`, which were the Linear layers that acted on individual heads.\n",
    "\n",
    "The code in this section, defines a new SAE with standard architectue that has block-diagonal encoder and decoder weights. In the process, we\n",
    "- normalize the dictionary vectors to have unit norm.\n",
    "- rescale weights by `norm_scaling_factor` as the SAEs were trained with normalized activations. (Specifically we set `cfg.normalize_activations=\"expected_average_only_in\"`). SAELens has a `fold_activation_norm_scaling_factor` function to fold the overall scaling factor into the weights of a trained SAE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = \"checkpoints/d543gzxy/final_204800000/\"\n",
    "# W&B: https://wandb.ai/shehper/gpt2-small-attn-4-sae/runs/pumu7rz3?nw=nwusershehper\n",
    "\n",
    "# ckpt_dir = \"checkpoints/5eu9598y/final_409600000/\"\n",
    "# # W&B: https://wandb.ai/shehper/gpt2-small-attn-5-sae/runs/s4om7ilc?nw=nwusershehper\n",
    "\n",
    "device = \"cuda\"\n",
    "sae = SAE.load_from_pretrained(path=ckpt_dir,\n",
    "                                    device=device)\n",
    "orig_architecure = sae.cfg.architecture\n",
    "\n",
    "sae.W_dec = sae.get_W_dec()\n",
    "sae.b_dec = sae.get_b_dec()\n",
    "sae.W_enc = sae.get_W_enc()\n",
    "sae.b_enc = sae.get_b_enc()\n",
    "\n",
    "dec_norms = sae.W_dec.norm(dim=-1)\n",
    "sae.W_enc *= dec_norms\n",
    "sae.b_enc *= dec_norms\n",
    "sae.W_dec /= dec_norms[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Dataset is not tokenized. Pre-tokenizing will improve performance and allows for more control over special tokens. See https://jbloomaus.github.io/SAELens/training_saes/#pretokenizing-datasets for more info.\n"
     ]
    }
   ],
   "source": [
    "from sae_lens import ActivationsStore\n",
    "\n",
    "activations_store = ActivationsStore.from_sae(\n",
    "    model = model,\n",
    "    sae = sae,\n",
    "    streaming=True,\n",
    "    store_batch_size_prompts=8,\n",
    "    n_batches_in_buffer=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/128 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1217 > 1024). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 128/128 [00:03<00:00, 38.75it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "def get_tokens(\n",
    "    activations_store: ActivationsStore,\n",
    "    n_batches_to_sample_from: int = 4096 * 6,\n",
    "    n_prompts_to_select: int = 4096 * 6,\n",
    "):\n",
    "    all_tokens_list = []\n",
    "    pbar = tqdm(range(n_batches_to_sample_from))\n",
    "    for _ in pbar:\n",
    "        batch_tokens = activations_store.get_batch_tokens()\n",
    "        batch_tokens = batch_tokens[torch.randperm(batch_tokens.shape[0])][\n",
    "            : batch_tokens.shape[0]\n",
    "        ]\n",
    "        all_tokens_list.append(batch_tokens)\n",
    "\n",
    "    all_tokens = torch.cat(all_tokens_list, dim=0)\n",
    "    all_tokens = all_tokens[torch.randperm(all_tokens.shape[0])]\n",
    "    return all_tokens[:n_prompts_to_select]\n",
    "\n",
    "# 1000 prompts is plenty for a demo.\n",
    "token_dataset = get_tokens(activations_store, 128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Estimating norm scaling factor: 100%|██████████| 30/30 [00:03<00:00,  8.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# Some SAEs will require we estimate the activation norm and fold it into the weights. This is easy with SAE Lens. \n",
    "if sae.cfg.normalize_activations:\n",
    "    norm_scaling_factor = activations_store.estimate_norm_scaling_factor(n_batches_for_norm_estimate=30)\n",
    "    sae.fold_activation_norm_scaling_factor(norm_scaling_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "sae_sd = OrderedDict()\n",
    "\n",
    "# sae_sd = sae.state_dict()\n",
    "sae_sd[\"W_dec\"] = sae.W_dec\n",
    "sae_sd[\"b_dec\"] = sae.b_dec\n",
    "sae_sd[\"W_enc\"] = sae.W_enc \n",
    "sae_sd[\"b_enc\"] = sae.b_enc\n",
    "\n",
    "new_cfg = sae.cfg\n",
    "new_cfg.architecture = \"standard\"\n",
    "\n",
    "new_sae = SAE(cfg=new_cfg)\n",
    "new_sae.load_state_dict(state_dict=sae_sd)\n",
    "del sae\n",
    "sae = new_sae"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Text Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for loading text dataset is borrowed from Connor Kissane's [attention-output-saes](https://github.com/ckkissane/attention-output-saes) repository.\n",
    "\n",
    "The second cell will take ~5 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "\n",
    "def get_batch_tokens(dataset_iter, batch_size, model):\n",
    "    tokens = []\n",
    "    total_tokens = 0\n",
    "    seq_len = 1024\n",
    "    while total_tokens < batch_size*seq_len:\n",
    "        try:\n",
    "            # Retrieve next item from iterator\n",
    "            row = next(dataset_iter)[\"text\"]\n",
    "        except StopIteration:\n",
    "            # Break the loop if dataset ends\n",
    "            break\n",
    "        \n",
    "        # Tokenize the text with a check for max_length\n",
    "        cur_toks = model.to_tokens(row)\n",
    "        tokens.append(cur_toks)\n",
    "        \n",
    "        total_tokens += cur_toks.numel()\n",
    "\n",
    "    # Check if any tokens were collected\n",
    "    if not tokens:\n",
    "        return None\n",
    "\n",
    "    # Depending on your model's tokenization, you might need to pad the tokens here\n",
    "\n",
    "    # Flatten the list of tokens\n",
    "    flat_tokens = torch.cat(tokens, dim=-1).flatten()\n",
    "    flat_tokens = flat_tokens[:batch_size * seq_len]\n",
    "    reshaped_tokens = einops.rearrange(\n",
    "        flat_tokens,\n",
    "        \"(batch seq_len) -> batch seq_len\",\n",
    "        batch=batch_size,\n",
    "    )\n",
    "    reshaped_tokens[:, 0] = model.tokenizer.bos_token_id\n",
    "    return reshaped_tokens\n",
    "\n",
    "# def shuffle_data(all_tokens):\n",
    "#     print(\"Shuffled data\")\n",
    "#     return all_tokens[torch.randperm(all_tokens.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached data from disk\n",
      "torch.Size([48828, 1024])\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\n",
    "    path = \"Skylion007/openwebtext\",\n",
    "    split=\"train\",\n",
    "    streaming=True,\n",
    ")\n",
    "\n",
    "\n",
    "data_dir = \"/home/ubuntu/storage/data\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "dataset_iter = iter(dataset)\n",
    "num_tokens = 5e7\n",
    "seq_len = 1024\n",
    "all_tokens_batches = int(num_tokens) // seq_len\n",
    "try:\n",
    "    print(\"Loading cached data from disk\")\n",
    "    all_tokens = torch.load(f\"{data_dir}/owt_tokens_reshaped.pt\")\n",
    "    # all_tokens = shuffle_data(all_tokens)\n",
    "    print(all_tokens.shape)\n",
    "except:\n",
    "    print(\"Data was not cached: Loading data first time\")\n",
    "    all_tokens = get_batch_tokens(dataset_iter, all_tokens_batches, model)\n",
    "    torch.save(all_tokens, f\"{data_dir}/owt_tokens_reshaped.pt\")\n",
    "    print(\"all_tokens.shape\", all_tokens.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Dashboards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9840411c58c4fbc8f8d0b82b651060f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Forward passes to cache data for vis:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88919b0bca0d4ea8a526b4a9622b0c33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting vis data from cached data:   0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Task                                           </span>┃<span style=\"font-weight: bold\"> Time   </span>┃<span style=\"font-weight: bold\"> Pct % </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│ (1) Initialization                             │ 0.00s  │ 0.0%  │\n",
       "│ (2) Forward passes to gather model activations │ 22.80s │ 32.0% │\n",
       "│ (3) Computing feature acts from model acts     │ 31.64s │ 44.4% │\n",
       "│ (4) Getting data for tables                    │ 0.04s  │ 0.1%  │\n",
       "│ (5) Getting data for histograms                │ 0.30s  │ 0.4%  │\n",
       "│ (6) Getting data for sequences                 │ 16.48s │ 23.1% │\n",
       "│ (7) Getting data for quantiles                 │ 0.00s  │ 0.0%  │\n",
       "└────────────────────────────────────────────────┴────────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTask                                          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mTime  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mPct %\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│ (1) Initialization                             │ 0.00s  │ 0.0%  │\n",
       "│ (2) Forward passes to gather model activations │ 22.80s │ 32.0% │\n",
       "│ (3) Computing feature acts from model acts     │ 31.64s │ 44.4% │\n",
       "│ (4) Getting data for tables                    │ 0.04s  │ 0.1%  │\n",
       "│ (5) Getting data for histograms                │ 0.30s  │ 0.4%  │\n",
       "│ (6) Getting data for sequences                 │ 16.48s │ 23.1% │\n",
       "│ (7) Getting data for quantiles                 │ 0.00s  │ 0.0%  │\n",
       "└────────────────────────────────────────────────┴────────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sae_vis.data_config_classes import SaeVisConfig\n",
    "from sae_vis.data_storing_fns import SaeVisData\n",
    "\n",
    "# test_feature_idx_gpt = [2048 * i + j for i in [1, 5, 6] for j in range(10)]\n",
    "test_feature_idx_gpt = [2048 * i + j for i in range(12) for j in range(10)]\n",
    "\n",
    "hook_name = sae.cfg.hook_name\n",
    "\n",
    "feature_vis_config_gpt = SaeVisConfig(\n",
    "    hook_point=hook_name,\n",
    "    features=test_feature_idx_gpt,\n",
    "    batch_size=2048,\n",
    "    minibatch_size_tokens=32,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "sae_vis_data_gpt = SaeVisData.create(\n",
    "    encoder=sae,\n",
    "    model=model, # type: ignore\n",
    "    tokens= all_tokens, # token_dataset[:100000][\"tokens\"],  # type: ignore\n",
    "    cfg=feature_vis_config_gpt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da5730d763f446139d8953d04db0bf3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving feature-centric vis:   0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90b531e951914399baa81ed437506225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving feature-centric vis:   0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42d71ca8fb614e79b7d95117fd839d5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving feature-centric vis:   0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcd152af4a4e4d9f9d6b6bf69ff32135",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving feature-centric vis:   0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_47654/1631618840.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_feature_idx_gpt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{dir_name}/{feature}_jb_{sae.cfg.hook_layer}_{orig_architecure}_owt_n_seqs_{all_tokens.shape[0]}_new.html\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0msae_vis_data_gpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_feature_centric_vis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/storage/sae_vis/sae_vis/data_storing_fns.py\u001b[0m in \u001b[0;36msave_feature_centric_vis\u001b[0;34m(self, filename, feature_idx)\u001b[0m\n\u001b[1;32m   1088\u001b[0m         \u001b[0;31m# (we arbitarily set the HTML string to be the HTML string for the first feature's view; they're all the same)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1090\u001b[0;31m             html_obj = feature_data._get_html_data_feature_centric(\n\u001b[0m\u001b[1;32m   1091\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_centric_layout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m             )\n",
      "\u001b[0;32m~/storage/sae_vis/sae_vis/data_storing_fns.py\u001b[0m in \u001b[0;36m_get_html_data_feature_centric\u001b[0;34m(self, layout, decode_fn)\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mcomponent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_component_from_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m                 html_obj += component._get_html_data(\n\u001b[0m\u001b[1;32m    861\u001b[0m                     \u001b[0mcfg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcomponent_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m                     \u001b[0mdecode_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/storage/sae_vis/sae_vis/data_storing_fns.py\u001b[0m in \u001b[0;36m_get_html_data\u001b[0;34m(self, cfg, decode_fn, id_suffix, column, component_specific_kwargs)\u001b[0m\n\u001b[1;32m    747\u001b[0m             \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_group_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m         ):\n\u001b[0;32m--> 749\u001b[0;31m             html_obj += sequences_group._get_html_data(\n\u001b[0m\u001b[1;32m    750\u001b[0m                 \u001b[0mcfg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m                 \u001b[0mdecode_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/storage/sae_vis/sae_vis/data_storing_fns.py\u001b[0m in \u001b[0;36m_get_html_data\u001b[0;34m(self, cfg, decode_fn, id_suffix, column, component_specific_kwargs)\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m         \u001b[0;31m# Accumulate the HTML data for each sequence in this group\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mgroup_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m             html_obj += seq._get_html_data(\n\u001b[1;32m    647\u001b[0m                 \u001b[0mcfg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dir_name = \"jb_features\"\n",
    "os.makedirs(dir_name, exist_ok=True)\n",
    "orig_architecure = \"block_diag\"\n",
    "for feature in test_feature_idx_gpt:\n",
    "    filename = f\"{dir_name}/{feature}_jb_{sae.cfg.hook_layer}_{orig_architecure}_owt_n_seqs_{all_tokens.shape[0]}_new.html\"\n",
    "    sae_vis_data_gpt.save_feature_centric_vis(filename, feature)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DFA by source position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reducing the size for my poor Laptop's small memory :(\n",
    "all_tokens = all_tokens[:4].clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cache = model.run_with_cache(all_tokens)\n",
    "layer = 4\n",
    "v = cache[\"v\", layer] # (B, T, nh, dh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights = cache[\"pattern\", layer] # (B, nh, T, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_sum_Av = attn_weights.unsqueeze(dim=-1) * v.transpose(dim0=1, dim1=2).unsqueeze(dim=2) # (B, nh, T, T, dh)\n",
    "pre_sum_Av_cat = pre_sum_Av.permute(dims=(0, 2, 3, 1, 4)).flatten(start_dim=-2, end_dim=-1) # (B, T, T, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_id = 0\n",
    "dfa_src = pre_sum_Av_cat @ sae.W_enc[:, feature_id] # (B, T, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do it for more than one features at a time, let feature_id be a list of feature ids\n",
    "# then dfa_src will have shape (B, T, T, H) for H being the number of features\n",
    "\n",
    "# As this computation can be done for a batch of features, we can perhaps do it inside the _get_feature_data function of sae_vis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
