{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "lr_decay_steps: 400\n",
      "l1_warmup_steps: 100\n",
      "Run name: 16384-L1-2-LR-5e-05-Tokens-8.192e+06\n",
      "n_tokens_per_buffer (millions): 1.048576\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.001024\n",
      "Total training steps: 2000\n",
      "Total wandb updates: 40\n",
      "n_tokens_per_feature_sampling_window (millions): 4194.304\n",
      "n_tokens_per_dead_feature_window (millions): 4194.304\n",
      "We will reset the sparsity calculation 2 times.\n",
      "Number tokens in sparsity calculation window: 4.10e+06\n",
      "Total Training Tokens: 8192000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from sae_lens import LanguageModelSAERunnerConfig, SAETrainingRunner\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# default config values\n",
    "model_name = \"gelu-2l\" \n",
    "dataset_path = \"NeelNanda/c4-code-tokenized-2b\"\n",
    "\n",
    "total_training_steps = 2_000 \n",
    "batch_size = 4096 \n",
    "new_cached_activations_path = (\n",
    "    f\"./cached_activations/{model_name}/{dataset_path}/{total_training_steps}\"\n",
    ")\n",
    "\n",
    "hook_layer=1\n",
    "hook_name=f\"blocks.{hook_layer}.attn.hook_z\"\n",
    "hook_head_index = None\n",
    "l1_coefficients = [2, 5, 10]\n",
    "d_in= 512\n",
    "expansion_factor = 32\n",
    "\n",
    "total_training_tokens = total_training_steps * batch_size\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "lr_warm_up_steps = 0\n",
    "lr_decay_steps = total_training_steps // 5  # 20% of training steps.\n",
    "print(f\"lr_decay_steps: {lr_decay_steps}\")\n",
    "l1_warmup_steps = total_training_steps // 20  # 5% of training steps.\n",
    "print(f\"l1_warmup_steps: {l1_warmup_steps}\")\n",
    "log_to_wandb = False\n",
    "\n",
    "for l1_coefficient in l1_coefficients:\n",
    "    cfg = LanguageModelSAERunnerConfig(\n",
    "        # Pick a tiny model to make this easier.\n",
    "        model_name=model_name, \n",
    "        ## MLP Layer 0 ##\n",
    "        hook_name=hook_name, \n",
    "        hook_layer=hook_layer, \n",
    "        hook_head_index=hook_head_index,\n",
    "        d_in=d_in,\n",
    "        dataset_path=dataset_path,\n",
    "        streaming=True, # pre-download the token dataset\n",
    "        context_size=1024,\n",
    "        is_dataset_tokenized=True,\n",
    "        prepend_bos=True,\n",
    "\n",
    "        n_heads=8, # TODO: replace the hack\n",
    "\n",
    "        # How big do we want our SAE to be?\n",
    "        expansion_factor=expansion_factor,\n",
    "        # Dataset / Activation Store\n",
    "        # When we do a proper test\n",
    "        # training_tokens= 820_000_000, # 200k steps * 4096 batch size ~ 820M tokens (doable overnight on an A100)\n",
    "        # For now.\n",
    "        use_cached_activations=False,\n",
    "        #cached_activations_path=\"./gelu-2l\",\n",
    "        training_tokens=total_training_tokens,  # For initial testing I think this is a good number.\n",
    "        train_batch_size_tokens=batch_size,\n",
    "        # Loss Function\n",
    "        ## Reconstruction Coefficient.\n",
    "        mse_loss_normalization=None,  # MSE Loss Normalization is not mentioned (so we use stanrd MSE Loss). But not we take an average over the batch.\n",
    "        ## Anthropic does not mention using an Lp norm other than L1.\n",
    "        l1_coefficient=l1_coefficient,\n",
    "        lp_norm=1.0,\n",
    "        # Instead, they multiply the L1 loss contribution\n",
    "        # from each feature of the activations by the decoder norm of the corresponding feature.\n",
    "        scale_sparsity_penalty_by_decoder_norm=True,\n",
    "        # Learning Rate\n",
    "        lr_scheduler_name=\"constant\",  # we set this independently of warmup and decay steps. # TODO: understand why it's constant\n",
    "        l1_warm_up_steps=l1_warmup_steps,\n",
    "        lr_warm_up_steps=lr_warm_up_steps,\n",
    "        lr_decay_steps=lr_warm_up_steps,\n",
    "        ## No ghost grad term.\n",
    "        use_ghost_grads=False,\n",
    "        # Initialization / Architecture\n",
    "        architecture=\"block_diag\",\n",
    "        apply_b_dec_to_input=False,\n",
    "        # encoder bias zero's. (I'm not sure what it is by default now)\n",
    "        # decoder bias zero's.\n",
    "        b_dec_init_method=\"zeros\",\n",
    "        normalize_sae_decoder=False,\n",
    "        decoder_heuristic_init=True,\n",
    "        init_encoder_as_decoder_transpose=True,\n",
    "        # Optimizer\n",
    "        lr=5e-5,\n",
    "        ## adam optimizer has no weight decay by default so worry about this.\n",
    "        adam_beta1=0.9,\n",
    "        adam_beta2=0.999,\n",
    "        # Buffer details won't matter in we cache / shuffle our activations ahead of time.\n",
    "        n_batches_in_buffer=64,\n",
    "        store_batch_size_prompts=16,\n",
    "        normalize_activations=\"expected_average_only_in\", # TODO: not sure what's the best choice # Activation Normalization Strategy. Either none, expected_average_only_in, or constant_norm_rescale.\n",
    "        # Feature Store\n",
    "        feature_sampling_window=1000,\n",
    "        dead_feature_window=1000,\n",
    "        dead_feature_threshold=1e-4,\n",
    "        # WANDB\n",
    "        log_to_wandb=log_to_wandb,  # always use wandb unless you are just testing code.\n",
    "        wandb_project=f\"{model_name}-attn-{hook_layer}-sae\",\n",
    "        wandb_log_frequency=50,\n",
    "        eval_every_n_wandb_logs=10,\n",
    "        # Misc\n",
    "        device=device,\n",
    "        seed=42,\n",
    "        n_checkpoints=2,\n",
    "        checkpoint_path=\"checkpoints\",\n",
    "        dtype=\"float32\",\n",
    "    )\n",
    "\n",
    "    print(f\"Total Training Tokens: {total_training_tokens}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gelu-2l into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "620a0af0d77b4426890857cdd8f19d23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# look at the next cell to see some instruction for what to do while this is running.\n",
    "torch.manual_seed(42)\n",
    "runner = SAETrainingRunner(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1024, 16384])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = 16\n",
    "T = 1024\n",
    "n_heads = 8\n",
    "d_heads = 64\n",
    "torch.manual_seed(34)\n",
    "x = torch.randn(B, T, n_heads * d_heads, device=runner.sae.device)\n",
    "encode_out1, _ = runner.sae.encode_block_diag(x)\n",
    "encode_out1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1024, 16384])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_out2 = runner.sae.encode_block_diag2(x)\n",
    "encode_out2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(encode_out1, encode_out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1024, 512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_out1 = runner.sae.decode_block_diag(encode_out1)\n",
    "decode_out1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1024, 512])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_out2 = runner.sae.decode_block_diag2(encode_out1)\n",
    "decode_out2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "torch.allclose(decode_out1, decode_out2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right, all of the encoder and decoder blocks are in sae.parameters(), but W_dec and W_enc are themselves not parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(init_enc_block0_weight, trained_sae.enc_blocks[\"0\"].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.2146, -0.2100,  0.2851,  ...,  0.2254, -0.2738,  0.2695],\n",
       "        [-0.2679, -0.3336,  0.4122,  ...,  0.2343, -0.3494,  0.2086],\n",
       "        [-0.2055, -0.3342,  0.4172,  ...,  0.3241, -0.3312,  0.2757],\n",
       "        ...,\n",
       "        [-0.2709, -0.2905,  0.2231,  ...,  0.4075, -0.2526,  0.2426],\n",
       "        [-0.2616, -0.3662,  0.2116,  ...,  0.2071, -0.1918,  0.3365],\n",
       "        [-0.0830, -0.3156,  0.2708,  ...,  0.2718,  0.2263,  0.0888]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_sae.enc_blocks[\"0\"].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0956,  0.1038, -0.0293,  ..., -0.0853,  0.0385, -0.0430],\n",
       "        [ 0.0383, -0.0260,  0.1037,  ..., -0.0729, -0.0428, -0.0987],\n",
       "        [ 0.1048, -0.0248,  0.1075,  ...,  0.0134, -0.0221, -0.0373],\n",
       "        ...,\n",
       "        [ 0.0353,  0.0179, -0.0835,  ...,  0.1018,  0.0542, -0.0653],\n",
       "        [ 0.0442, -0.0610, -0.0666,  ..., -0.0071,  0.1038,  0.0466],\n",
       "        [-0.0996, -0.0305, -0.0282,  ..., -0.0201, -0.0599,  0.0170]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_enc_block0_weight = runner.sae.enc_blocks[\"0\"].weight.data.clone()\n",
    "print(type(runner.sae.enc_blocks[\"0\"].weight))\n",
    "init_enc_block0_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0211,  0.0075, -0.0008,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.0076, -0.0160, -0.0113,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0147, -0.0185,  0.0202,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ..., -0.0030,  0.0142, -0.0211],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0172, -0.0179,  0.0185],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ..., -0.0214,  0.0196,  0.0009]],\n",
       "       device='cuda:0', grad_fn=<BlockDiagBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_sae.W_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0211,  0.0075, -0.0008,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.0076, -0.0160, -0.0113,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0147, -0.0185,  0.0202,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ..., -0.0030,  0.0142, -0.0211],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0172, -0.0179,  0.0185],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ..., -0.0214,  0.0196,  0.0009]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_W_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.2814,  0.2926,  0.3111,  ...,  0.2991,  0.2828,  0.2396],\n",
       "        [ 0.3061,  0.2845,  0.2847,  ...,  0.3076,  0.3029,  0.2191],\n",
       "        [-0.2998, -0.3089, -0.2818,  ..., -0.2952, -0.3136, -0.3017],\n",
       "        ...,\n",
       "        [-0.3106, -0.2902, -0.3064,  ..., -0.2969, -0.3158, -0.2894],\n",
       "        [ 0.2977,  0.2948,  0.2994,  ...,  0.3026,  0.3081, -0.2788],\n",
       "        [-0.3063, -0.2794, -0.3147,  ..., -0.3104, -0.2895, -0.3027]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_sae.dec_blocks[\"0\"].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "init_W_dec = runner.sae.W_dec.data.clone()\n",
    "init_W_enc = runner.sae.W_enc.data.clone()\n",
    "print(type(runner.sae.W_dec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshehper\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/storage/attn_saes/wandb/run-20240706_022807-u62jq9ob</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shehper/gelu-2l-attn-1-sae/runs/u62jq9ob' target=\"_blank\">16384-L1-2-LR-5e-05-Tokens-2.458e+07</a></strong> to <a href='https://wandb.ai/shehper/gelu-2l-attn-1-sae' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shehper/gelu-2l-attn-1-sae' target=\"_blank\">https://wandb.ai/shehper/gelu-2l-attn-1-sae</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shehper/gelu-2l-attn-1-sae/runs/u62jq9ob' target=\"_blank\">https://wandb.ai/shehper/gelu-2l-attn-1-sae/runs/u62jq9ob</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Estimating norm scaling factor: 100%|██████████| 1000/1000 [00:35<00:00, 28.09it/s]\n",
      "6000| MSE Loss 558.173 | L1 747.712: 100%|██████████| 24576000/24576000 [07:45<00:00, 52841.55it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "387dc53272724a009a4cd950edd6c0b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='16.268 MB of 16.268 MB uploaded (0.002 MB deduped)\\r'), FloatProgress(value=1.0, m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>details/current_l1_coefficient</td><td>▁▅██████████████████████████████████████</td></tr><tr><td>details/current_learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>details/n_training_tokens</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>losses/auxiliary_reconstruction_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/ghost_grad_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/l1_loss</td><td>▁▅▆▇▇███████████████████████████████████</td></tr><tr><td>losses/mse_loss</td><td>▁▄▆▇▇███████▇███▇███████▇██▇█████▇█▇████</td></tr><tr><td>losses/overall_loss</td><td>▁▅▇▇████████████████████████████████████</td></tr><tr><td>metrics/CE_loss_score</td><td>▇▇▁▂▆▅▄▅▇█▂▃</td></tr><tr><td>metrics/ce_loss_with_ablation</td><td>▅▁▂▁▆▄▃▃▇▅█▂</td></tr><tr><td>metrics/ce_loss_with_sae</td><td>▄▁▃▂▆▄▃▃▆▄█▃</td></tr><tr><td>metrics/ce_loss_without_sae</td><td>▁▄▃▃▆▄▄▃▅█▂▁</td></tr><tr><td>metrics/explained_variance</td><td>█▇▅▄▄▂▃▂▃▃▂▃▁▃▃▂▂▂▃▃▂▂▂▁▂▂▃▂▂▂▂▂▂▁▂▂▂▂▂▂</td></tr><tr><td>metrics/explained_variance_std</td><td>▁▄▅▄▄▆▅▆▄▄▆▄▄▄▃▇▄▅▄▄▅█▅▇▆▆▃▅▆▅▆▅▆▆▆▅▅▆▄▅</td></tr><tr><td>metrics/l0</td><td>█▅▅▃▂▂▃▂▁▃▃▁▄▂▃▅▅▄▄▄▂▃▃▃▄▁▂▃▄▃▂▃▃▅▃▅▄▃▅▃</td></tr><tr><td>metrics/l2_norm</td><td>▂▅▂▂▄▆▂▁▃█▃▂</td></tr><tr><td>metrics/l2_norm_in</td><td>▁▅▂▂▄▆▃▁▃█▃▂</td></tr><tr><td>metrics/l2_ratio</td><td>█▅▇▇▅▅▄█▄▁▆▅</td></tr><tr><td>metrics/mean_log10_feature_sparsity</td><td>▁█▇▄▅▄</td></tr><tr><td>sparsity/below_1e-5</td><td>▁▁▁▁▁▁</td></tr><tr><td>sparsity/below_1e-6</td><td>▁▁▁▁▁▁</td></tr><tr><td>sparsity/dead_features</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sparsity/mean_passes_since_fired</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>details/current_l1_coefficient</td><td>2</td></tr><tr><td>details/current_learning_rate</td><td>5e-05</td></tr><tr><td>details/n_training_tokens</td><td>24576000</td></tr><tr><td>losses/auxiliary_reconstruction_loss</td><td>0.0</td></tr><tr><td>losses/ghost_grad_loss</td><td>0.0</td></tr><tr><td>losses/l1_loss</td><td>373.85617</td></tr><tr><td>losses/mse_loss</td><td>558.17346</td></tr><tr><td>losses/overall_loss</td><td>1305.88574</td></tr><tr><td>metrics/CE_loss_score</td><td>-0.00654</td></tr><tr><td>metrics/ce_loss_with_ablation</td><td>6.99475</td></tr><tr><td>metrics/ce_loss_with_sae</td><td>7.01973</td></tr><tr><td>metrics/ce_loss_without_sae</td><td>3.27516</td></tr><tr><td>metrics/explained_variance</td><td>-0.58011</td></tr><tr><td>metrics/explained_variance_std</td><td>0.31837</td></tr><tr><td>metrics/l0</td><td>8245.59375</td></tr><tr><td>metrics/l2_norm</td><td>2.70948</td></tr><tr><td>metrics/l2_norm_in</td><td>22.19877</td></tr><tr><td>metrics/l2_ratio</td><td>0.12225</td></tr><tr><td>metrics/mean_log10_feature_sparsity</td><td>-0.3877</td></tr><tr><td>sparsity/below_1e-5</td><td>0</td></tr><tr><td>sparsity/below_1e-6</td><td>0</td></tr><tr><td>sparsity/dead_features</td><td>0</td></tr><tr><td>sparsity/mean_passes_since_fired</td><td>0.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">16384-L1-2-LR-5e-05-Tokens-2.458e+07</strong> at: <a href='https://wandb.ai/shehper/gelu-2l-attn-1-sae/runs/u62jq9ob' target=\"_blank\">https://wandb.ai/shehper/gelu-2l-attn-1-sae/runs/u62jq9ob</a><br/> View project at: <a href='https://wandb.ai/shehper/gelu-2l-attn-1-sae' target=\"_blank\">https://wandb.ai/shehper/gelu-2l-attn-1-sae</a><br/>Synced 5 W&B file(s), 0 media file(s), 6 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240706_022807-u62jq9ob/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trained_sae = runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0211,  0.0075, -0.0008,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.0076, -0.0160, -0.0113,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0147, -0.0185,  0.0202,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ..., -0.0030,  0.0142, -0.0211],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0172, -0.0179,  0.0185],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ..., -0.0214,  0.0196,  0.0009]],\n",
       "       device='cuda:0', grad_fn=<BlockDiagBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_sae.W_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(init_W_dec, trained_sae.W_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(init_W_enc, trained_sae.W_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0597,  0.0812, -0.0701,  ...,  0.0253, -0.1019, -0.1229],\n",
       "       device='cuda:0', grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_sae.b_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.3798e-03, -1.4406e-02,  6.6600e-03,  1.7949e-02, -2.0928e-02,\n",
       "        -1.7106e-02, -6.7341e-03, -1.4137e-02,  1.8258e-02,  7.8433e-03,\n",
       "        -1.6477e-02, -6.5140e-03,  6.9009e-03, -6.8866e-04,  2.0495e-02,\n",
       "        -7.3730e-03,  1.2815e-03, -4.3822e-03, -4.4309e-04,  1.5135e-02,\n",
       "        -2.4795e-03, -1.4174e-02,  2.8641e-03, -1.9982e-02,  9.7295e-03,\n",
       "        -8.5921e-03, -1.9043e-02,  1.8825e-02,  2.1587e-02,  9.4444e-03,\n",
       "         1.0079e-02, -9.3242e-03, -1.9717e-02, -1.8062e-02, -8.3509e-04,\n",
       "         1.3807e-02,  3.9873e-03,  1.5570e-02, -6.1561e-03, -3.0117e-03,\n",
       "         1.1046e-03,  1.3812e-02,  1.8806e-03,  2.1446e-02, -1.1533e-02,\n",
       "         8.5836e-04,  1.5602e-02,  1.7486e-02, -4.0159e-03,  8.7399e-04,\n",
       "        -5.6304e-03,  6.2776e-03,  1.8152e-02,  1.1937e-02, -1.7574e-02,\n",
       "        -1.2467e-02, -8.6007e-03, -1.5669e-03,  1.9530e-03, -1.4182e-02,\n",
       "         1.6515e-02, -8.2939e-03,  4.3075e-03, -7.4483e-03, -9.9832e-03,\n",
       "         1.5400e-02, -7.8276e-03,  1.8333e-02,  1.6065e-02,  1.5794e-02,\n",
       "        -1.0174e-02,  9.8620e-03,  9.9325e-03,  1.0042e-02,  4.4841e-03,\n",
       "        -5.2967e-03,  1.5967e-02,  6.3166e-03, -1.6304e-02,  1.8903e-03,\n",
       "         1.2348e-02,  1.5531e-02,  8.5566e-03, -1.0786e-02,  1.8300e-03,\n",
       "        -1.5902e-02,  2.5025e-03, -6.1975e-03, -1.7365e-02, -2.8119e-03,\n",
       "         2.1122e-03,  9.5513e-03,  1.0145e-02, -8.0543e-03, -1.9302e-02,\n",
       "        -1.1572e-02,  2.3927e-03,  4.7062e-03,  9.8842e-03,  1.4323e-02,\n",
       "         3.0709e-04,  9.3984e-03, -6.2569e-03, -2.0111e-02, -6.1484e-03,\n",
       "        -5.3214e-03, -1.6251e-02, -1.4313e-02,  1.9002e-02, -1.8564e-02,\n",
       "         2.1892e-02,  4.6396e-03,  1.5375e-02,  1.9315e-03, -2.2052e-02,\n",
       "        -9.1604e-03,  1.2456e-02,  2.1052e-02,  1.1110e-02, -1.3480e-02,\n",
       "        -8.4249e-03, -5.8377e-03, -2.0687e-02, -1.0632e-02,  4.2289e-03,\n",
       "        -4.0224e-03,  9.8036e-03, -5.2902e-03,  3.2402e-03,  1.6000e-02,\n",
       "         1.3813e-02, -2.2331e-03, -9.8773e-03,  2.0342e-02,  5.2654e-03,\n",
       "         7.7689e-03,  2.0118e-02, -1.9179e-02,  1.4944e-02, -1.6937e-02,\n",
       "        -3.8523e-03,  1.6567e-02,  9.0003e-03,  2.1995e-02, -3.7272e-03,\n",
       "         9.5056e-03, -9.5755e-03, -2.6574e-03, -6.5964e-03, -2.0274e-02,\n",
       "        -1.9307e-02, -1.6825e-02,  6.8318e-03,  2.1220e-02, -1.0944e-02,\n",
       "        -1.6717e-02, -1.0474e-02,  1.6347e-03,  1.5871e-02,  8.3941e-03,\n",
       "        -9.5250e-03,  1.0702e-02,  3.0422e-03,  4.0025e-03, -2.7169e-03,\n",
       "         2.3218e-05, -1.9618e-02,  8.2327e-03,  1.1971e-02, -7.8233e-03,\n",
       "         1.2879e-02,  1.9737e-02, -4.4564e-03, -9.4954e-03, -8.5579e-03,\n",
       "        -1.6524e-02,  4.7672e-03, -1.7111e-02,  4.7194e-03,  2.0389e-02,\n",
       "         3.1561e-03,  1.4239e-02, -8.4904e-03,  1.8696e-02,  9.1066e-03,\n",
       "         1.7961e-02,  1.3336e-02,  2.1227e-02, -6.8242e-03,  1.9864e-02,\n",
       "        -1.2982e-02,  6.0987e-03, -1.7159e-04,  1.0737e-02,  1.9988e-02,\n",
       "        -1.9674e-02, -1.4067e-02,  2.1439e-02, -1.5247e-02, -1.6049e-02,\n",
       "        -2.0818e-02,  1.9314e-02, -1.8986e-02, -1.0852e-03, -9.6789e-03,\n",
       "         9.9067e-03, -1.7160e-02,  2.0734e-02,  1.4270e-02, -5.9109e-03,\n",
       "        -1.7561e-03, -1.4378e-02,  5.9075e-03,  7.9174e-03, -1.2532e-03,\n",
       "         3.0260e-03,  1.8184e-02,  2.0534e-02,  1.6875e-02,  1.9888e-02,\n",
       "         2.0053e-03, -9.6780e-03, -2.1501e-02,  1.5586e-03, -1.3306e-02,\n",
       "         1.3959e-02,  1.9013e-02, -1.9707e-03, -1.9667e-02,  1.5015e-02,\n",
       "        -1.6600e-03, -4.4312e-03, -2.0383e-02, -1.5306e-02, -1.4258e-02,\n",
       "         1.5534e-02,  1.2864e-02,  1.9599e-03, -1.6064e-02,  1.6158e-02,\n",
       "        -5.5141e-03,  1.9992e-02,  7.2639e-03, -1.2270e-02, -8.0061e-03,\n",
       "        -9.2400e-03,  1.0232e-03, -5.5459e-03,  1.5202e-02,  1.7318e-02,\n",
       "         4.3657e-03,  3.3346e-03, -5.1637e-03, -1.9638e-03, -2.0044e-02,\n",
       "        -2.4226e-03, -7.2147e-03,  1.6145e-02, -2.1189e-02, -1.5856e-02,\n",
       "         9.3124e-03,  5.8605e-03, -5.6209e-03,  1.9574e-02,  9.9533e-03,\n",
       "        -1.9006e-02, -1.8161e-02,  1.5324e-02, -1.0391e-03, -2.2002e-02,\n",
       "        -1.6478e-02,  5.9831e-03,  1.2950e-02,  9.7132e-03,  9.5872e-03,\n",
       "        -6.1155e-03, -7.2511e-03, -1.7441e-02, -1.8641e-03, -3.3536e-03,\n",
       "         4.2144e-03, -1.8365e-02, -2.1714e-02, -5.5033e-03, -1.8914e-02,\n",
       "        -5.3334e-03, -2.0262e-02, -1.2233e-02, -5.8446e-03,  1.2813e-02,\n",
       "        -1.2751e-02,  1.7549e-02, -5.6615e-03,  1.7716e-02,  3.7918e-03,\n",
       "         1.8072e-02,  7.1753e-03,  1.3258e-02,  9.5446e-03, -2.2465e-03,\n",
       "         5.4168e-03, -1.1699e-02,  9.9525e-03, -4.6533e-03, -2.6558e-03,\n",
       "         3.6673e-03, -1.1969e-02, -1.3481e-03,  2.1016e-03, -1.8687e-02,\n",
       "        -8.9294e-03, -1.3973e-02, -8.5371e-03,  1.5835e-02, -1.5910e-02,\n",
       "        -4.5298e-03, -1.5502e-02, -1.0866e-03,  7.7001e-03,  2.1108e-02,\n",
       "         1.9575e-02, -2.1446e-02, -4.0207e-03,  4.7460e-03,  1.8204e-02,\n",
       "        -1.6487e-02,  1.0613e-03, -7.6683e-03,  1.6119e-03, -9.2235e-03,\n",
       "        -1.4506e-02, -9.4459e-03,  1.2910e-02, -2.1640e-03,  2.8931e-04,\n",
       "         1.3879e-02, -1.8240e-04, -9.7415e-03, -3.5712e-03,  2.6609e-03,\n",
       "         1.1811e-02, -1.3570e-02,  4.1479e-03,  9.5230e-03,  1.9572e-02,\n",
       "        -1.2814e-02,  3.8382e-03, -2.0996e-02,  9.7720e-03, -4.7013e-03,\n",
       "         2.0195e-02, -4.9933e-03,  4.5380e-04, -5.6294e-03, -1.0777e-02,\n",
       "        -1.9261e-02,  4.5820e-04,  1.2266e-02,  2.9061e-03, -4.5473e-03,\n",
       "         1.6227e-02, -1.2922e-02,  1.7519e-02,  9.1659e-03, -1.3584e-02,\n",
       "        -2.0271e-02,  8.3985e-03, -1.4149e-03,  9.7899e-03,  3.2304e-03,\n",
       "         2.1818e-02, -1.9427e-02, -1.1162e-02,  1.8065e-02, -9.9713e-03,\n",
       "         1.3727e-02, -8.0284e-03,  2.2017e-02, -5.7520e-03, -1.5552e-02,\n",
       "         1.2710e-02, -1.4272e-02, -1.5429e-02,  3.4578e-03, -1.2700e-02,\n",
       "        -1.8065e-02,  4.7884e-05,  1.3811e-02, -1.8473e-02,  1.6903e-03,\n",
       "         1.6596e-02,  8.9170e-03,  2.2040e-03, -2.0003e-02,  4.2393e-03,\n",
       "         2.0441e-02, -1.6321e-02, -1.8887e-02, -1.2713e-02,  8.0847e-03,\n",
       "        -3.9080e-03,  4.0378e-03,  1.5901e-02,  1.5924e-02, -2.6757e-04,\n",
       "        -1.7951e-02,  1.1294e-02, -1.5901e-02,  1.1642e-02, -5.7592e-03,\n",
       "         1.9497e-02, -2.1072e-02,  1.2698e-03, -1.0614e-02,  3.9841e-03,\n",
       "        -2.3999e-03,  1.3248e-02, -1.9742e-02,  1.9520e-02,  1.9328e-02,\n",
       "         1.9681e-02, -2.8547e-03,  3.1495e-03, -9.8730e-03, -1.7304e-02,\n",
       "        -2.0303e-02, -2.0322e-02, -2.3543e-03,  8.6592e-03, -2.7236e-04,\n",
       "        -2.0222e-02,  8.4615e-03,  2.1180e-03,  4.2674e-03,  2.5592e-03,\n",
       "         2.0719e-02, -1.1642e-02,  9.3864e-03,  1.4032e-02,  7.8961e-04,\n",
       "        -1.3747e-02,  1.0109e-02, -1.3050e-02,  1.5522e-03, -1.5257e-02,\n",
       "        -1.5443e-02, -5.5612e-03,  1.7496e-02, -2.0138e-02,  7.2611e-03,\n",
       "         4.2105e-05,  5.7584e-03, -1.9806e-02,  1.2392e-02, -1.1605e-02,\n",
       "         2.2086e-02, -1.3747e-03,  2.0813e-02, -2.1645e-02,  7.5019e-03,\n",
       "        -1.1381e-02,  1.0674e-02,  5.5110e-03, -2.1907e-02, -1.1889e-02,\n",
       "         1.3309e-02, -9.1529e-03,  1.1930e-02, -7.7401e-03, -7.1287e-03,\n",
       "        -5.4505e-03, -1.0425e-02, -1.0578e-02, -1.5516e-03,  1.6415e-02,\n",
       "         4.9655e-03,  1.4018e-02,  1.1418e-02,  1.6611e-02, -1.5351e-02,\n",
       "         2.4973e-03, -1.8569e-02, -1.7925e-02, -1.2065e-02, -5.8810e-03,\n",
       "        -4.1335e-03,  1.3426e-02, -8.2989e-03, -1.6010e-02,  1.8705e-03,\n",
       "         2.1569e-02,  1.4459e-02,  1.1350e-02, -6.5487e-04,  6.4808e-03,\n",
       "        -1.9302e-02, -1.5611e-02,  2.1814e-03,  8.9492e-03,  1.6430e-02,\n",
       "         1.4322e-02,  2.6433e-03,  2.9280e-03, -3.9739e-03,  1.2040e-02,\n",
       "        -6.3524e-03,  7.5688e-03, -1.8060e-02,  1.4731e-02,  2.0151e-02,\n",
       "        -1.6987e-03,  6.2111e-03], device='cuda:0', grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_sae.b_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trained_sae.W_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.parameter.Parameter"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trained_sae.b_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16384])\n",
      "torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "for p in trained_sae.parameters():\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.parameter.Parameter"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trained_sae.enc_blocks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gelu-2l into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5be76ba445bd41caa037d75a2ef3c2bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_heads': 8, 'architecture': 'block_diag', 'd_in': 512, 'd_sae': 16384, 'activation_fn_str': 'relu', 'activation_fn_kwargs': {}, 'apply_b_dec_to_input': False, 'dtype': 'float32', 'model_name': 'gelu-2l', 'hook_name': 'blocks.1.attn.hook_z', 'hook_layer': 1, 'hook_head_index': None, 'device': 'cuda', 'context_size': 1024, 'prepend_bos': True, 'finetuning_scaling_factor': False, 'normalize_activations': 'expected_average_only_in', 'dataset_path': 'NeelNanda/c4-code-tokenized-2b', 'dataset_trust_remote_code': True, 'sae_lens_training_version': '3.11.0'}\n"
     ]
    }
   ],
   "source": [
    "# look at the next cell to see some instruction for what to do while this is running.\n",
    "torch.manual_seed(42)\n",
    "new_runner = SAETrainingRunner(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.parameter.Parameter"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(new_runner.sae.b_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "type(torch.block_diag(nn.Parameter(torch.randn(2, 2)), nn.Parameter(torch.randn(2, 2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16384])\n",
      "torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "for p in new_runner.sae.parameters():\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
